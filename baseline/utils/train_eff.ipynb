{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9806c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import layers\n",
    "import os.path\n",
    "from tensorflow import data as tf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3566 files belonging to 5 classes.\n",
      "Found 894 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224  # Input size for EfficientNetB0\n",
    "MODEL_FILE = \"model_eff.h5\"\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"split_data/train\",\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=32,\n",
    "    label_mode='int',\n",
    "    shuffle=True\n",
    ")\n",
    "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"split_data/val\",\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=32,\n",
    "    label_mode='int',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train.class_names # ['Pallas_cats', 'Persian_cats', 'Ragdolls', 'Singapura_cats', 'Sphynx_cats']\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.2)\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess(x, y, train):\n",
    "    x = tf.cast(x, tf.float32) \n",
    "    x = preprocess_input(x)  # EfficientNetB0 specific preprocessing\n",
    "    if train:\n",
    "        x = augmentation(x)  # Apply data augmentation\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8beb8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train.map(lambda x, y: preprocess(x, y, True)).prefetch(buffer_size=AUTOTUNE)\n",
    "val_generator = val.map(lambda x, y: preprocess(x, y, False)).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee677ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "    )\n",
    "    base_model.trainable = False  # 初始冻结基础模型\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    x = preprocess_input(inputs)  # 关键：使用专用预处理\n",
    "    x = base_model(x, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a047dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing(model_file):\n",
    "    model = load_model(model_file)\n",
    "    # 解冻最后4个块进行微调\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.Model):  # 找到基础模型\n",
    "            base_model = layer\n",
    "            break\n",
    "    \n",
    "    if base_model:\n",
    "        # 解冻最后4个块\n",
    "        for layer in base_model.layers[-20:]:\n",
    "            if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                layer.trainable = True\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b9008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_file, train_path, validation_path, num_classes=5, steps=100, num_epochs=20):\n",
    "    if os.path.exists(model_file):\n",
    "        print(\"\\n*** Loading existing model ***\\n\")\n",
    "        model = load_existing(model_file)\n",
    "        # 必须重新编译\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=1e-4),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n*** Creating new model ***\\n\")\n",
    "        model = create_model(num_classes)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=1e-3),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_file,\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    # 第一阶段：训练新添加的层\n",
    "    print(\"=== Phase 1: Training Head ===\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=[checkpoint],\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=20\n",
    "    )\n",
    "\n",
    "    # 第二阶段：微调\n",
    "    print(\"\\n=== Phase 2: Fine-Tuning ===\")\n",
    "    # 找到基础模型并解冻部分层\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.Model):\n",
    "            base_model = layer\n",
    "            break\n",
    "    \n",
    "    if base_model:\n",
    "        base_model.trainable = True\n",
    "        # 解冻最后4个块 (EfficientNetB0有7个块，解冻block5b到block7a)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False  # 先冻结所有\n",
    "            \n",
    "        # 解冻最后部分层\n",
    "        for layer in base_model.layers[-20:]:\n",
    "            if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                layer.trainable = True\n",
    "\n",
    "    # 使用更小的学习率\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-5),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=[checkpoint],\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=20\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde3287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Creating new model ***\n",
      "\n",
      "=== Phase 1: Training Head ===\n",
      "Epoch 1/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.7340 - loss: 0.7134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 249ms/step - accuracy: 0.7349 - loss: 0.7110 - val_accuracy: 0.9328 - val_loss: 0.2106\n",
      "Epoch 2/15\n",
      "\u001b[1m 12/100\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.8855 - loss: 0.3330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\48946\\anaconda3\\envs\\dlenv\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.8829 - loss: 0.3426 - val_accuracy: 0.9328 - val_loss: 0.2113\n",
      "Epoch 3/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 236ms/step - accuracy: 0.9021 - loss: 0.2570 - val_accuracy: 0.9297 - val_loss: 0.1981\n",
      "Epoch 4/15\n",
      "\u001b[1m 12/100\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 182ms/step - accuracy: 0.8940 - loss: 0.2981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9007 - loss: 0.2819 - val_accuracy: 0.9422 - val_loss: 0.1786\n",
      "Epoch 5/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 238ms/step - accuracy: 0.9194 - loss: 0.2213 - val_accuracy: 0.9328 - val_loss: 0.1961\n",
      "Epoch 6/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.9188 - loss: 0.2393 - val_accuracy: 0.9328 - val_loss: 0.2002\n",
      "Epoch 7/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 236ms/step - accuracy: 0.9158 - loss: 0.2182 - val_accuracy: 0.9406 - val_loss: 0.1763\n",
      "Epoch 8/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.8838 - loss: 0.2706 - val_accuracy: 0.9391 - val_loss: 0.1923\n",
      "Epoch 9/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 245ms/step - accuracy: 0.9223 - loss: 0.2123 - val_accuracy: 0.9375 - val_loss: 0.1916\n",
      "Epoch 10/15\n",
      "\u001b[1m 12/100\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 187ms/step - accuracy: 0.8947 - loss: 0.2348"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.8960 - loss: 0.2319 - val_accuracy: 0.9484 - val_loss: 0.1658\n",
      "Epoch 11/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 243ms/step - accuracy: 0.9244 - loss: 0.1820 - val_accuracy: 0.9406 - val_loss: 0.1837\n",
      "Epoch 12/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9126 - loss: 0.2609 - val_accuracy: 0.9391 - val_loss: 0.2079\n",
      "Epoch 13/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 242ms/step - accuracy: 0.9374 - loss: 0.1780 - val_accuracy: 0.9344 - val_loss: 0.1972\n",
      "Epoch 14/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - accuracy: 0.9139 - loss: 0.2365 - val_accuracy: 0.9453 - val_loss: 0.1752\n",
      "Epoch 15/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 242ms/step - accuracy: 0.9506 - loss: 0.1521 - val_accuracy: 0.9344 - val_loss: 0.2167\n",
      "\n",
      "=== Phase 2: Fine-Tuning ===\n",
      "Epoch 1/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 262ms/step - accuracy: 0.9490 - loss: 0.1443 - val_accuracy: 0.9375 - val_loss: 0.2053\n",
      "Epoch 2/15\n",
      "\u001b[1m 12/100\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 200ms/step - accuracy: 0.9491 - loss: 0.1342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - accuracy: 0.9458 - loss: 0.1431 - val_accuracy: 0.9516 - val_loss: 0.1573\n",
      "Epoch 3/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9462 - loss: 0.1530"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 253ms/step - accuracy: 0.9462 - loss: 0.1529 - val_accuracy: 0.9547 - val_loss: 0.1544\n",
      "Epoch 4/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9487 - loss: 0.1343 - val_accuracy: 0.9453 - val_loss: 0.1551\n",
      "Epoch 5/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 251ms/step - accuracy: 0.9533 - loss: 0.1263 - val_accuracy: 0.9516 - val_loss: 0.1587\n",
      "Epoch 6/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9587 - loss: 0.1532 - val_accuracy: 0.9531 - val_loss: 0.1605\n",
      "Epoch 7/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 255ms/step - accuracy: 0.9442 - loss: 0.1389 - val_accuracy: 0.9547 - val_loss: 0.1475\n",
      "Epoch 8/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9427 - loss: 0.1498 - val_accuracy: 0.9531 - val_loss: 0.1701\n",
      "Epoch 9/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 251ms/step - accuracy: 0.9511 - loss: 0.1359 - val_accuracy: 0.9375 - val_loss: 0.1946\n",
      "Epoch 10/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - accuracy: 0.9500 - loss: 0.1291 - val_accuracy: 0.9469 - val_loss: 0.1869\n",
      "Epoch 11/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 257ms/step - accuracy: 0.9510 - loss: 0.1309 - val_accuracy: 0.9422 - val_loss: 0.1955\n",
      "Epoch 12/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - accuracy: 0.9406 - loss: 0.1600 - val_accuracy: 0.9500 - val_loss: 0.1813\n",
      "Epoch 13/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 266ms/step - accuracy: 0.9463 - loss: 0.1379 - val_accuracy: 0.9516 - val_loss: 0.1702\n",
      "Epoch 14/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.9582 - loss: 0.1397 - val_accuracy: 0.9500 - val_loss: 0.1778\n",
      "Epoch 15/15\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 268ms/step - accuracy: 0.9640 - loss: 0.1119 - val_accuracy: 0.9531 - val_loss: 0.1622\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train(\n",
    "        MODEL_FILE,\n",
    "        train_path=\"split_data/train\",\n",
    "        validation_path=\"split_data/val\",\n",
    "        steps=100,  # 根据数据集大小调整\n",
    "        num_epochs=15\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
